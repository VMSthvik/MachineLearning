{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "080dcd44",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0ab197",
   "metadata": {},
   "source": [
    "The essence of Natural Language Processing lies in making computers understand the natural language. That’s not an easy task though. Computers can understand the structured form of data like spreadsheets and the tables in the database, but human languages, texts, and voices form an unstructured category of data, and it gets difficult for the computer to understand it, and there arises the need for Natural Language Processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091326d3",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dce85b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e5beec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', 'abc.zip', 'alpino', 'alpino.zip', 'biocreative_ppi', 'biocreative_ppi.zip', 'brown', 'brown.zip', 'brown_tei', 'brown_tei.zip', 'cess_cat', 'cess_cat.zip', 'cess_esp', 'cess_esp.zip', 'chat80', 'chat80.zip', 'city_database', 'city_database.zip', 'cmudict', 'cmudict.zip', 'comparative_sentences', 'comparative_sentences.zip', 'comtrans.zip', 'conll2000', 'conll2000.zip', 'conll2002', 'conll2002.zip', 'conll2007.zip', 'crubadan', 'crubadan.zip', 'dependency_treebank', 'dependency_treebank.zip', 'dolch', 'dolch.zip', 'europarl_raw', 'europarl_raw.zip', 'floresta', 'floresta.zip', 'framenet_v15', 'framenet_v15.zip', 'framenet_v17', 'framenet_v17.zip', 'gazetteers', 'gazetteers.zip', 'genesis', 'genesis.zip', 'gutenberg', 'gutenberg.zip', 'ieer', 'ieer.zip', 'inaugural', 'inaugural.zip', 'indian', 'indian.zip', 'jeita.zip', 'kimmo', 'kimmo.zip', 'knbc.zip', 'lin_thesaurus', 'lin_thesaurus.zip', 'machado.zip', 'mac_morpho', 'mac_morpho.zip', 'masc_tagged.zip', 'movie_reviews', 'movie_reviews.zip', 'mte_teip5', 'mte_teip5.zip', 'names', 'names.zip', 'nombank.1.0.zip', 'nonbreaking_prefixes', 'nonbreaking_prefixes.zip', 'nps_chat', 'nps_chat.zip', 'omw', 'omw.zip', 'opinion_lexicon', 'opinion_lexicon.zip', 'panlex_swadesh.zip', 'paradigms', 'paradigms.zip', 'pil', 'pil.zip', 'pl196x', 'pl196x.zip', 'ppattach', 'ppattach.zip', 'problem_reports', 'problem_reports.zip', 'product_reviews_1', 'product_reviews_1.zip', 'product_reviews_2', 'product_reviews_2.zip', 'propbank.zip', 'pros_cons', 'pros_cons.zip', 'ptb', 'ptb.zip', 'qc', 'qc.zip', 'reuters.zip', 'rte', 'rte.zip', 'semcor.zip', 'senseval', 'senseval.zip', 'sentence_polarity', 'sentence_polarity.zip', 'sentiwordnet', 'sentiwordnet.zip', 'shakespeare', 'shakespeare.zip', 'sinica_treebank', 'sinica_treebank.zip', 'smultron', 'smultron.zip', 'state_union', 'state_union.zip', 'stopwords', 'stopwords.zip', 'subjectivity', 'subjectivity.zip', 'swadesh', 'swadesh.zip', 'switchboard', 'switchboard.zip', 'timit', 'timit.zip', 'toolbox', 'toolbox.zip', 'treebank', 'treebank.zip', 'twitter_samples', 'twitter_samples.zip', 'udhr', 'udhr.zip', 'udhr2', 'udhr2.zip', 'unicode_samples', 'unicode_samples.zip', 'universal_treebanks_v20.zip', 'verbnet', 'verbnet.zip', 'verbnet3', 'verbnet3.zip', 'webtext', 'webtext.zip', 'wordnet', 'wordnet.zip', 'wordnet31', 'wordnet31.zip', 'wordnet_ic', 'wordnet_ic.zip', 'words', 'words.zip', 'ycoe', 'ycoe.zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(nltk.data.find(\"corpora\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9aa156f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7591f48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff0c215b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')\n",
    "hamlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54687cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ The Tragedie of Hamlet by William Shakespeare 1599 ] Actus Primus . Scoena Prima . Enter Barnardo and Francisco two Centinels . Barnardo . Who ' s there ? Fran . Nay answer me : Stand & vnfold your selfe Bar . Long liue the King Fran . Barnardo ? Bar . He Fran . You come most carefully vpon your houre Bar . ' Tis now strook twelue , get thee to bed Francisco Fran . For this releefe much thankes : ' Tis bitter cold , And I am sicke at heart Barn . Haue you had quiet Guard ? Fran . Not a Mouse stirring Barn . Well , goodnight . If you do meet Horatio and Marcellus , the Riuals of my Watch , bid them make hast . Enter Horatio and Marcellus . Fran . I thinke I heare them . Stand : who ' s there ? Hor . Friends to this ground Mar . And Leige - men to the Dane Fran . Giue you good night Mar . O farwel honest Soldier , who hath relieu ' d you ? Fra . Barnardo ha ' s my place : giue you goodnight . Exit Fran . Mar . Holla Barnardo Bar . Say , what is Horatio there ? Hor . A peece of him Bar . Welcome Horatio , welcome good Marcellus Mar . What , ha ' s this thing appear ' d againe to night Bar . I haue seene nothing Mar . Horatio saies , ' tis but our Fantasie , And will not let beleefe take hold of him Touching this dreaded sight , twice seene of vs , Therefore I haue intreated him along With vs , to watch the minutes of this Night , That if againe this Apparition come , He may approue our eyes , and speake to it Hor . Tush , tush , ' twill not appeare Bar . Sit downe a - while , And let vs once againe assaile your eares , That are so fortified against our Story , What we two Nights haue seene Hor . Well , sit we downe , And let vs heare Barnardo speake of this Barn . Last night of all , When yond same Starre that ' s Westward from the Pole Had made his course t ' illume that part of Heauen Where now it burnes , Marcellus and my selfe , The Bell then beating one Mar . Peace , breake thee of : Enter the Ghost . Looke where it comes againe Barn . In the same figure , like the King that ' s dead Mar . Thou art a Scholler ; speake to it Horatio Barn . Lookes it not like the King ? Marke it Horatio Hora . Most like : It harrowes me with fear & wonder Barn . It would be spoke too Mar . Question it Horatio Hor . What art "
     ]
    }
   ],
   "source": [
    "for word in hamlet[:500]:\n",
    "    print(word, sep = ' ', end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f89ee9",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07cfe85",
   "metadata": {},
   "source": [
    "Tokenization is one of the most common tasks when it comes to working with text data. But what does the term ‘tokenization’ actually mean?\n",
    "\n",
    "Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e84de0d",
   "metadata": {},
   "source": [
    "# Importing the library for word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "507a3c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c10b2396",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI =\"\"\"Alan Turing, a brilliant mathematician, who broke the Nazi encryption machine Enigma, came up with a history-changing question, “Can machines think?” in 1950. The actual research began in 1956, at a conference held at Dartmouth College (a lot of the inventions have come into the picture, thanks to the Ivy League). A couple of attendees at the conference were the ones who came up with the idea and also the name “Artificial Intelligence”. But since the whole idea was new, people didn’t buy the idea and funding for further research was pulled off. This period, the 1950s – 1980s was called “AI Winter”. In the early 1980s however, the Japanese government saw a future in AI and started funding the field again. As this was interconnected to the electronics and computer science fields, there was a sudden spike in those as well. The first AI machine was introduced to the world in 1997; IBM’s Deep Blue became the first computer to beat a chess champion when it defeated Russian grandmaster Garry Kasparov. And that, my dear readers, was the advent of a massive field called “AI”.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daf616bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76efab16",
   "metadata": {},
   "source": [
    "# Performing word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "869c46e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alan',\n",
       " 'Turing',\n",
       " ',',\n",
       " 'a',\n",
       " 'brilliant',\n",
       " 'mathematician',\n",
       " ',',\n",
       " 'who',\n",
       " 'broke',\n",
       " 'the',\n",
       " 'Nazi',\n",
       " 'encryption',\n",
       " 'machine',\n",
       " 'Enigma',\n",
       " ',',\n",
       " 'came',\n",
       " 'up',\n",
       " 'with',\n",
       " 'a',\n",
       " 'history-changing',\n",
       " 'question',\n",
       " ',',\n",
       " '“',\n",
       " 'Can',\n",
       " 'machines',\n",
       " 'think',\n",
       " '?',\n",
       " '”',\n",
       " 'in',\n",
       " '1950',\n",
       " '.',\n",
       " 'The',\n",
       " 'actual',\n",
       " 'research',\n",
       " 'began',\n",
       " 'in',\n",
       " '1956',\n",
       " ',',\n",
       " 'at',\n",
       " 'a',\n",
       " 'conference',\n",
       " 'held',\n",
       " 'at',\n",
       " 'Dartmouth',\n",
       " 'College',\n",
       " '(',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'the',\n",
       " 'inventions',\n",
       " 'have',\n",
       " 'come',\n",
       " 'into',\n",
       " 'the',\n",
       " 'picture',\n",
       " ',',\n",
       " 'thanks',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Ivy',\n",
       " 'League',\n",
       " ')',\n",
       " '.',\n",
       " 'A',\n",
       " 'couple',\n",
       " 'of',\n",
       " 'attendees',\n",
       " 'at',\n",
       " 'the',\n",
       " 'conference',\n",
       " 'were',\n",
       " 'the',\n",
       " 'ones',\n",
       " 'who',\n",
       " 'came',\n",
       " 'up',\n",
       " 'with',\n",
       " 'the',\n",
       " 'idea',\n",
       " 'and',\n",
       " 'also',\n",
       " 'the',\n",
       " 'name',\n",
       " '“',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " '”',\n",
       " '.',\n",
       " 'But',\n",
       " 'since',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'idea',\n",
       " 'was',\n",
       " 'new',\n",
       " ',',\n",
       " 'people',\n",
       " 'didn',\n",
       " '’',\n",
       " 't',\n",
       " 'buy',\n",
       " 'the',\n",
       " 'idea',\n",
       " 'and',\n",
       " 'funding',\n",
       " 'for',\n",
       " 'further',\n",
       " 'research',\n",
       " 'was',\n",
       " 'pulled',\n",
       " 'off',\n",
       " '.',\n",
       " 'This',\n",
       " 'period',\n",
       " ',',\n",
       " 'the',\n",
       " '1950s',\n",
       " '–',\n",
       " '1980s',\n",
       " 'was',\n",
       " 'called',\n",
       " '“',\n",
       " 'AI',\n",
       " 'Winter',\n",
       " '”',\n",
       " '.',\n",
       " 'In',\n",
       " 'the',\n",
       " 'early',\n",
       " '1980s',\n",
       " 'however',\n",
       " ',',\n",
       " 'the',\n",
       " 'Japanese',\n",
       " 'government',\n",
       " 'saw',\n",
       " 'a',\n",
       " 'future',\n",
       " 'in',\n",
       " 'AI',\n",
       " 'and',\n",
       " 'started',\n",
       " 'funding',\n",
       " 'the',\n",
       " 'field',\n",
       " 'again',\n",
       " '.',\n",
       " 'As',\n",
       " 'this',\n",
       " 'was',\n",
       " 'interconnected',\n",
       " 'to',\n",
       " 'the',\n",
       " 'electronics',\n",
       " 'and',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'fields',\n",
       " ',',\n",
       " 'there',\n",
       " 'was',\n",
       " 'a',\n",
       " 'sudden',\n",
       " 'spike',\n",
       " 'in',\n",
       " 'those',\n",
       " 'as',\n",
       " 'well',\n",
       " '.',\n",
       " 'The',\n",
       " 'first',\n",
       " 'AI',\n",
       " 'machine',\n",
       " 'was',\n",
       " 'introduced',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'in',\n",
       " '1997',\n",
       " ';',\n",
       " 'IBM',\n",
       " '’',\n",
       " 's',\n",
       " 'Deep',\n",
       " 'Blue',\n",
       " 'became',\n",
       " 'the',\n",
       " 'first',\n",
       " 'computer',\n",
       " 'to',\n",
       " 'beat',\n",
       " 'a',\n",
       " 'chess',\n",
       " 'champion',\n",
       " 'when',\n",
       " 'it',\n",
       " 'defeated',\n",
       " 'Russian',\n",
       " 'grandmaster',\n",
       " 'Garry',\n",
       " 'Kasparov',\n",
       " '.',\n",
       " 'And',\n",
       " 'that',\n",
       " ',',\n",
       " 'my',\n",
       " 'dear',\n",
       " 'readers',\n",
       " ',',\n",
       " 'was',\n",
       " 'the',\n",
       " 'advent',\n",
       " 'of',\n",
       " 'a',\n",
       " 'massive',\n",
       " 'field',\n",
       " 'called',\n",
       " '“',\n",
       " 'AI',\n",
       " '”',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI_tokens = word_tokenize(AI)\n",
    "AI_tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5925ebde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02d5cfe",
   "metadata": {},
   "source": [
    "For finding the occurance of lowercase words.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1da336db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6560c43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 20, ',': 12, 'a': 9, '.': 9, 'was': 7, 'in': 6, 'and': 5, '“': 4, '”': 4, 'to': 4, ...})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in AI_tokens:\n",
    "    fdist[word.lower()]+=1\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93458750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5732421d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34fd6df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 20), (',', 12), ('a', 9), ('.', 9), ('was', 7)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_top5 = fdist.most_common(5)\n",
    "fdist_top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76b6a7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alan Turing, a brilliant mathematician, who broke the Nazi encryption machine Enigma, came up with a history-changing question, “Can machines think?” in 1950. The actual research began in 1956, at a conference held at Dartmouth College (a lot of the inventions have come into the picture, thanks to the Ivy League). A couple of attendees at the conference were the ones who came up with the idea and also the name “Artificial Intelligence”. But since the whole idea was new, people didn’t buy the idea and funding for further research was pulled off. This period, the 1950s – 1980s was called “AI Winter”. In the early 1980s however, the Japanese government saw a future in AI and started funding the field again. As this was interconnected to the electronics and computer science fields, there was a sudden spike in those as well. The first AI machine was introduced to the world in 1997; IBM’s Deep Blue became the first computer to beat a chess champion when it defeated Russian grandmaster Garry Kasparov. And that, my dear readers, was the advent of a massive field called “AI”.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import blankline_tokenize\n",
    "AI_blank = blankline_tokenize(AI)\n",
    "AI_blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b253dc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_blank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3848bb61",
   "metadata": {},
   "source": [
    "# Ngrams, Bigrams and Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1f2363",
   "metadata": {},
   "source": [
    "1) Tokens of any number of consecutive written words are called Ngrams.\n",
    "\n",
    "2) Tokens of two consecutive written words are called Bigrams.\n",
    "\n",
    "3) Tokens of three consecutive written words are calles Trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29e08003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "935c7036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'basic',\n",
       " 'function',\n",
       " 'of',\n",
       " 'the',\n",
       " 'algorithms',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'is',\n",
       " 'data',\n",
       " 'analysis',\n",
       " '.',\n",
       " 'Let',\n",
       " 'me',\n",
       " 'put',\n",
       " 'it',\n",
       " 'this',\n",
       " 'way',\n",
       " '.',\n",
       " 'How',\n",
       " 'do',\n",
       " 'you',\n",
       " 'think',\n",
       " 'human',\n",
       " 'beings',\n",
       " 'learn',\n",
       " 'new',\n",
       " 'things',\n",
       " '?',\n",
       " 'They',\n",
       " 'observe',\n",
       " '.',\n",
       " 'They',\n",
       " 'observe',\n",
       " 'and',\n",
       " 'that',\n",
       " '’',\n",
       " 's',\n",
       " 'how',\n",
       " 'they',\n",
       " 'learn',\n",
       " '.',\n",
       " 'Machines',\n",
       " 'learn',\n",
       " 'the',\n",
       " 'same',\n",
       " 'way',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"The basic function of the algorithms of AI is data analysis. Let me put it this way. How do you think human beings learn new things? They observe. They observe and that’s how they learn. Machines learn the same way. \"\n",
    "quotes_tokens = nltk.word_tokenize(string)\n",
    "quotes_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "749eb892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'basic'),\n",
       " ('basic', 'function'),\n",
       " ('function', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'algorithms'),\n",
       " ('algorithms', 'of'),\n",
       " ('of', 'AI'),\n",
       " ('AI', 'is'),\n",
       " ('is', 'data'),\n",
       " ('data', 'analysis'),\n",
       " ('analysis', '.'),\n",
       " ('.', 'Let'),\n",
       " ('Let', 'me'),\n",
       " ('me', 'put'),\n",
       " ('put', 'it'),\n",
       " ('it', 'this'),\n",
       " ('this', 'way'),\n",
       " ('way', '.'),\n",
       " ('.', 'How'),\n",
       " ('How', 'do'),\n",
       " ('do', 'you'),\n",
       " ('you', 'think'),\n",
       " ('think', 'human'),\n",
       " ('human', 'beings'),\n",
       " ('beings', 'learn'),\n",
       " ('learn', 'new'),\n",
       " ('new', 'things'),\n",
       " ('things', '?'),\n",
       " ('?', 'They'),\n",
       " ('They', 'observe'),\n",
       " ('observe', '.'),\n",
       " ('.', 'They'),\n",
       " ('They', 'observe'),\n",
       " ('observe', 'and'),\n",
       " ('and', 'that'),\n",
       " ('that', '’'),\n",
       " ('’', 's'),\n",
       " ('s', 'how'),\n",
       " ('how', 'they'),\n",
       " ('they', 'learn'),\n",
       " ('learn', '.'),\n",
       " ('.', 'Machines'),\n",
       " ('Machines', 'learn'),\n",
       " ('learn', 'the'),\n",
       " ('the', 'same'),\n",
       " ('same', 'way'),\n",
       " ('way', '.')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_bigrams = list(nltk.bigrams(quotes_tokens))\n",
    "quotes_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "523883a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'basic', 'function'),\n",
       " ('basic', 'function', 'of'),\n",
       " ('function', 'of', 'the'),\n",
       " ('of', 'the', 'algorithms'),\n",
       " ('the', 'algorithms', 'of'),\n",
       " ('algorithms', 'of', 'AI'),\n",
       " ('of', 'AI', 'is'),\n",
       " ('AI', 'is', 'data'),\n",
       " ('is', 'data', 'analysis'),\n",
       " ('data', 'analysis', '.'),\n",
       " ('analysis', '.', 'Let'),\n",
       " ('.', 'Let', 'me'),\n",
       " ('Let', 'me', 'put'),\n",
       " ('me', 'put', 'it'),\n",
       " ('put', 'it', 'this'),\n",
       " ('it', 'this', 'way'),\n",
       " ('this', 'way', '.'),\n",
       " ('way', '.', 'How'),\n",
       " ('.', 'How', 'do'),\n",
       " ('How', 'do', 'you'),\n",
       " ('do', 'you', 'think'),\n",
       " ('you', 'think', 'human'),\n",
       " ('think', 'human', 'beings'),\n",
       " ('human', 'beings', 'learn'),\n",
       " ('beings', 'learn', 'new'),\n",
       " ('learn', 'new', 'things'),\n",
       " ('new', 'things', '?'),\n",
       " ('things', '?', 'They'),\n",
       " ('?', 'They', 'observe'),\n",
       " ('They', 'observe', '.'),\n",
       " ('observe', '.', 'They'),\n",
       " ('.', 'They', 'observe'),\n",
       " ('They', 'observe', 'and'),\n",
       " ('observe', 'and', 'that'),\n",
       " ('and', 'that', '’'),\n",
       " ('that', '’', 's'),\n",
       " ('’', 's', 'how'),\n",
       " ('s', 'how', 'they'),\n",
       " ('how', 'they', 'learn'),\n",
       " ('they', 'learn', '.'),\n",
       " ('learn', '.', 'Machines'),\n",
       " ('.', 'Machines', 'learn'),\n",
       " ('Machines', 'learn', 'the'),\n",
       " ('learn', 'the', 'same'),\n",
       " ('the', 'same', 'way'),\n",
       " ('same', 'way', '.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_trigrams = list(nltk.trigrams(quotes_tokens))\n",
    "quotes_trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae245f",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb93d3c",
   "metadata": {},
   "source": [
    "Stemming involves normalizing a word into its base or root form.\n",
    "\n",
    "Ex: Consider the words: Affect, Affection, Affected, Affecting.\n",
    "The base or root form of the above words is \"Äffect.\"\n",
    "\n",
    "Note: The NLTK tool provides mainly three types of stemmers, namely:\n",
    "1) PorterStemmer\n",
    "\n",
    "2) LancasterStemmer\n",
    "\n",
    "3) SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc4e0213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72bc589b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem(\"Loving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2a8b6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Killing:kill\n",
      "Saving:save\n",
      "Protecting:protect\n",
      "Served:serv\n",
      "loved:love\n"
     ]
    }
   ],
   "source": [
    "words_to_stem = ['Killing', 'Saving', 'Protecting', 'Served', 'loved']\n",
    "\n",
    "for words in words_to_stem:\n",
    "    print(words + \":\" + pst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d029b601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Killing:kil\n",
      "Saving:sav\n",
      "Protecting:protect\n",
      "Served:serv\n",
      "loved:lov\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lst = LancasterStemmer()\n",
    "\n",
    "for words in words_to_stem:\n",
    "    print(words + \":\" + lst.stem(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cab24309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "sbst = SnowballStemmer('english') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5068ae8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Killing:kill\n",
      "Saving:save\n",
      "Protecting:protect\n",
      "Served:serv\n",
      "loved:love\n"
     ]
    }
   ],
   "source": [
    "for words in words_to_stem:\n",
    "    print(words + \":\" + sbst.stem(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556228d7",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298d7754",
   "metadata": {},
   "source": [
    "1) Groups together different inflicted forms of a word, called a lemma.\n",
    "\n",
    "2) Somehow similar to stemming as it maps words into one common root.\n",
    "\n",
    "3) The outcome of lemmatization is a proper word.\n",
    "\n",
    "for example, the words \"going\" and \"gone\" when lemmatized, should return \"go\" as the result.\n",
    "\n",
    "Lemmatization does acquire Wordnet database often for its functioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d5b6a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "word_lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4acf43e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpus'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lem.lemmatize('corpora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79812737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Killing:Killing\n",
      "Saving:Saving\n",
      "Protecting:Protecting\n",
      "Served:Served\n",
      "loved:loved\n"
     ]
    }
   ],
   "source": [
    "for words in words_to_stem:\n",
    "    print(words + \":\" + word_lem.lemmatize(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7db1bce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0593d5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "820b089f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 20), (',', 12), ('a', 9), ('.', 9), ('was', 7)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28fcfa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "punctuation = re.compile(r'[-.?!:;()|0-9]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f389a12",
   "metadata": {},
   "source": [
    "# Parts of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab130569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mama',\n",
       " 'is',\n",
       " 'a',\n",
       " 'natural',\n",
       " 'when',\n",
       " 'it',\n",
       " 'comes',\n",
       " 'to',\n",
       " 'kicking',\n",
       " 'ass',\n",
       " '.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Mama is a natural when it comes to kicking ass.\"\n",
    "sent_tokens = word_tokenize(sentence)\n",
    "len(sent_tokens)\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cbe45961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14ee9d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mama', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('a', 'DT')]\n",
      "[('natural', 'JJ')]\n",
      "[('when', 'WRB')]\n",
      "[('it', 'PRP')]\n",
      "[('comes', 'VBZ')]\n",
      "[('to', 'TO')]\n",
      "[('kicking', 'VBG')]\n",
      "[('ass', 'NN')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for token in sent_tokens:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "94c4f0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Elise', 'kicked', 'Arno', 'in', 'the', 'balls', '.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2 = \"Elise kicked Arno in the balls.\"\n",
    "sen2_tokens = word_tokenize(sent2)\n",
    "\n",
    "sen2_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e619c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Elise', 'NN')]\n",
      "[('kicked', 'VBN')]\n",
      "[('Arno', 'NN')]\n",
      "[('in', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('balls', 'NNS')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for token in sen2_tokens:\n",
    "    print(nltk.pos_tag([token]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee42aad",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed6424",
   "metadata": {},
   "source": [
    "The detection of a named entity which could be either a movie, a monetary value, a location or even a person is called Named Entity Recognition.\n",
    "\n",
    "We import ne_chunk to perform Named Entity Recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ce490151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b4adb540",
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"The US President resides in the White House.\"\n",
    "\n",
    "line_token = word_tokenize(line)\n",
    "\n",
    "line_tag = nltk.pos_tag(line_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a3c708da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Ghostscript executable isn't found.\n",
      "See http://web.mit.edu/ghostscript/www/Install.htm\n",
      "If you're using a Mac, you can try installing\n",
      "https://docs.brew.sh/Installation then `brew install ghostscript`\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    817\u001b[0m                     [\n\u001b[1;32m--> 818\u001b[1;33m                         find_binary(\n\u001b[0m\u001b[0;32m    819\u001b[0m                             \u001b[1;34m\"gs\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    687\u001b[0m ):\n\u001b[1;32m--> 688\u001b[1;33m     return next(\n\u001b[0m\u001b[0;32m    689\u001b[0m         find_binary_iter(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    672\u001b[0m     \"\"\"\n\u001b[1;32m--> 673\u001b[1;33m     for file in find_file_iter(\n\u001b[0m\u001b[0;32m    674\u001b[0m         \u001b[0mpath_to_bin\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"=\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\\n%s\\n%s\\n%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n===========================================================================",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    835\u001b[0m                 )\n\u001b[0;32m    836\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_error_message\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 837\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    838\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [('The', 'DT'), Tree('ORGANIZATION', [('US', 'NNP')]), ('President', 'NNP'), ('resides', 'VBZ'), ('in', 'IN'), ('the', 'DT'), Tree('FACILITY', [('White', 'NNP'), ('House', 'NNP')]), ('.', '.')])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_ner = ne_chunk(line_tag)\n",
    "line_ner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e13059e",
   "metadata": {},
   "source": [
    "# Syntax Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd299318",
   "metadata": {},
   "source": [
    "It is a tree repersentation of the syntactic structure of sentences or strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783a244e",
   "metadata": {},
   "source": [
    "Notebook author - Sathvik."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
